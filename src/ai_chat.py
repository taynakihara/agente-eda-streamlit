import streamlit as st
import pandas as pd
import numpy as np
import time
from utils.memoria_db import salvar_memoria, carregar_memoria

# Tentamos importar as libs de LLM, mas sem quebrar se n√£o estiverem instaladas
try:
    import openai
except ImportError:
    openai = None

try:
    import google.generativeai as genai
except ImportError:
    genai = None


# ==========================================
# üîπ Fun√ß√£o auxiliar: resumo do dataset (com cache)
# ==========================================
@st.cache_data(show_spinner=False)
def summarize_dataset(df: pd.DataFrame) -> str:
    """Resumo b√°sico do dataset (linhas, colunas, tipos, estat√≠sticas)."""
    if df is None or df.empty:
        return "Nenhum dado foi carregado."

    resumo = [f"O dataset possui {df.shape[0]} linhas e {df.shape[1]} colunas."]
    tipos = df.dtypes.value_counts().to_dict()
    tipos_texto = ", ".join([f"{k}: {v}" for k, v in tipos.items()])
    resumo.append(f"Tipos de dados ‚Üí {tipos_texto}.")

    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
    if numeric_cols:
        stats = df[numeric_cols].describe().T[["mean", "std", "min", "max"]]
        resumo.append("Estat√≠sticas resumidas das vari√°veis num√©ricas:")
        for col, row in stats.iterrows():
            resumo.append(
                f"‚Ä¢ {col}: m√©dia={row['mean']:.2f}, desvio={row['std']:.2f}, "
                f"min={row['min']:.2f}, max={row['max']:.2f}"
            )
    return "\n".join(resumo)


# ==========================================
# üîπ Mem√≥ria de Chat
# ==========================================
def initialize_memory():
    """Inicializa a mem√≥ria local e carrega as √∫ltimas conversas persistidas."""
    if "chat_history" not in st.session_state:
        st.session_state["chat_history"] = []
    if "dataset_summary" not in st.session_state:
        st.session_state["dataset_summary"] = None

    # Carregar hist√≥rico persistente
    try:
        memoria_salva = carregar_memoria(limit=5)
        if memoria_salva:
            for item in memoria_salva[::-1]:  # ordem cronol√≥gica
                st.session_state["chat_history"].append(
                    {
                        "role": "assistant",
                        "content": f"üïí {item['timestamp']}\n**{item['pergunta']}**\n\n{item['resposta']}",
                    }
                )
    except Exception as e:
        st.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel carregar mem√≥ria persistente: {e}")


def add_to_history(role: str, content: str):
    st.session_state["chat_history"].append({"role": role, "content": content})


def show_history():
    """Exibe o hist√≥rico do chat."""
    for message in st.session_state["chat_history"]:
        st.chat_message(message["role"]).markdown(message["content"])


# ==========================================
# üîπ Fun√ß√£o de resposta do agente
# ==========================================
def generate_response(
    prompt: str, dataset_summary: str, api_key: str, provider: str
) -> str:
    """Gera resposta real ou simulada de acordo com o provider."""
    if not api_key or not provider:
        return (
            f"üí¨ [Modo offline] Voc√™ perguntou: '{prompt}'\n\n{dataset_summary[:500]}"
        )

    # --- OPENAI ---
    if provider == "OpenAI" and openai:
        try:
            client = openai.OpenAI(api_key=api_key)
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": "Voc√™ √© um analista de dados √∫til e explicativo.",
                    },
                    {
                        "role": "user",
                        "content": f"{prompt}\n\n{dataset_summary[:1000]}",
                    },
                ],
                temperature=0.3,
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            return f"‚ö†Ô∏è Erro ao conectar √† API OpenAI: {e}"

    # --- GROQ ---
    elif provider == "Groq" and openai:
        try:
            client = openai.OpenAI(
                api_key=api_key, base_url="https://api.groq.com/openai/v1"
            )
            response = client.chat.completions.create(
                model="llama-3.1-70b-versatile",
                messages=[
                    {
                        "role": "system",
                        "content": "Voc√™ √© um analista de dados especializado em EDA.",
                    },
                    {
                        "role": "user",
                        "content": f"{prompt}\n\n{dataset_summary[:1000]}",
                    },
                ],
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            return f"‚ö†Ô∏è Erro ao conectar √† API Groq: {e}"

    # --- GEMINI ---
    elif provider == "Gemini" and genai:
        try:
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel("gemini-pro")
            response = model.generate_content(
                f"Usu√°rio: {prompt}\n\nContexto:\n{dataset_summary[:1000]}"
            )
            return response.text
        except Exception as e:
            return f"‚ö†Ô∏è Erro ao conectar √† API Gemini: {e}"

    return "‚ö†Ô∏è Nenhum provedor v√°lido configurado ou biblioteca ausente."


# ==========================================
# üîπ Fun√ß√£o principal do Chat
# ==========================================
def render_chat(
    data,
    numeric_cols,
    categorical_cols,
    dataset_summary=None,
    api_key=None,
    provider=None,
):
    st.markdown("### üí¨ Chat Interativo com Mem√≥ria Persistente")
    initialize_memory()
    show_history()

    # Entrada do usu√°rio
    user_input = st.chat_input("Digite sua pergunta sobre os dados...")
    if not user_input:
        return

    add_to_history("user", user_input)

    with st.chat_message("assistant"):
        placeholder = st.empty()
        placeholder.info("ü§ñ Processando sua pergunta...")

        try:
            response = generate_response(user_input, dataset_summary, api_key, provider)
        except Exception as e:
            response = f"‚ö†Ô∏è Erro ao processar sua solicita√ß√£o: {e}"
        finally:
            placeholder.empty()

        st.markdown(response)
        add_to_history("assistant", response)

        # üíæ Salvar pergunta e resposta na mem√≥ria persistente Supabase
        try:
            salvar_memoria(user_input, response, tipo_analise="chat")
        except Exception as e:
            st.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel salvar na mem√≥ria persistente: {e}")
